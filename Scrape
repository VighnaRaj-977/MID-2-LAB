import requests
from bs4 import BeautifulSoup
import pandas as pd

# Target website
url = "https://github.com/"

# Fetch webpage content
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# Extract news headlines and links
articles = soup.select('.athing')
data = []

for article in articles:
    title = article.select_one('.titleline a').text
    link = article.select_one('.titleline a')['href']
    data.append([title, link])

# Convert to DataFrame
df = pd.DataFrame(data, columns=['Title', 'Link'])

# Save CSV file inside cloned GitHub repo
csv_path = "/content/MID-2-LAB/hacker_news.csv"
df.to_csv(csv_path, index=False)

print(f"CSV file saved successfully at {csv_path}")
